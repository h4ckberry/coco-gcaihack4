1. システム概要
システム名: Janus / Contextual Finder

コンセプト: スマートフォンを「目・耳・口」とし、クラウド上のAIを「頭脳」、物理モーターを「体」とするサイバーフィジカルシステム（CPS）。

目的: ユーザーの身の回りの「探し物」や「状況確認」を、現在の視覚情報だけでなく、過去の文脈や因果関係を推論してサポートする、能動的なパートナーエージェントを実現する。

2. ターゲットユースケース (解決すべき課題)
システムは、単なる物体検知を超え、以下の状況に対応できなければならない。

CASE 1: 過去の記憶 (Memory)

「鍵がない！」→「昨日の朝、違うコートのポケットに入れていましたよ」と回答する。

CASE 2: 遮蔽と推論 (Reasoning)

「リモコンどこ？」→「さっき毛布を動かした時に、その下に隠れました」と推論する。

CASE 3: 微細な探索 (Fine-grained Search)

「指輪どこ置いたっけ？」→現在の映像を高解像度で解析し、コップの影にある小さな対象物を見つける。

CASE 4: リアルタイムの状況把握 (Real-time Context)

作業中、「あの工具どこ？」→散らかった机の上で、現在何の下敷きになっているかを即答する。

CASE 5: 他者の介入 (Agent Interference)

「スマホがない！」→「5分前にペットがくわえてクッションの下に隠しました」と、動く主体の行動を報告する。

3. 機能要件 (Functional Requirements)
F1: 知覚機能 (Perception)

フロントエンド(iOS)から定期的に送信される画像（例: 10秒間隔）を受け取ることができる。

ユーザーからの音声/テキストによる指示を受け取ることができる。

F2: 思考機能 (Cognition) - AI Agent core

マルチモーダル推論: 画像、テキスト、過去のログを組み合わせて状況を理解できる。

自律的な判断: ユーザーの指示が曖昧な場合、自律的に「質問する」「カメラを動かして確認する」「過去の記録を検索する」などのアクションを選択できる。

F3: 行動機能 (Action)

物理制御: 必要に応じて、obniz経由でサーボモーターを回転させ、カメラアングルを変更できる。

音声対話: AIの回答を、Cloud Text-to-Speechを用いて自然な音声データとしてフロントエンドに返却できる。

F4: 記憶機能 (Memory)

カメラに映った重要な物体やイベントを、時刻・場所と共に構造化データとしてFirestoreに記録し、後から検索可能にする (RAG)。

4. 非機能要件 (Non-Functional Requirements)
NF1: コスト効率性 (Crucial)

Cloud Storageの容量を圧迫しないよう、古い画像は自動的に削除されること（例: 2時間保持）。

無駄なAI処理を避けるため、画像に変化がない場合は重い処理をスキップする仕組みを持つこと。

AIモデルは、役割に応じて高性能モデル(Pro)と軽量モデル(Flash)を使い分けること。

NF2: スケーラビリティ

アイドル時はコンピュートリソースがゼロになるサーバーレス構成であること (Cloud Run)。

NF3: 応答性

「今どうなっている？」という問いに対しては、数秒以内に応答できること。